{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba46826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(r\"kaggle\\porto seguro\\train.csv\", index_col='id')\n",
    "test = pd.read_csv(r\"kaggle\\porto seguro\\test.csv\", index_col='id')\n",
    "submission = pd.read_csv(r\"kaggle\\porto seguro\\sample_submission.csv\", index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06effb27",
   "metadata": {},
   "source": [
    "### 피처 엔지니어링과 하이퍼파라미터 최적화 추가 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76bb6c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # 타깃값 제거\n",
    "all_features = all_data.columns # 전체 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df591a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 명목형 피처 추출\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3077c13",
   "metadata": {},
   "source": [
    "#### 1. 파생 피처 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd13cd0",
   "metadata": {},
   "source": [
    "- 결측값 개수를 파생 피처로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfe3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '데이터 하나당 결측값 개수'를 파생 피처로 추가\n",
    "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1eae69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_features = [feature for feature in all_features\n",
    "                     if ('cat' not in feature and 'calc' not in feature)]\n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d00f6",
   "metadata": {},
   "source": [
    "- 명목형 피처(원-핫 인코딩), calc 분류는 필요없는 피처라서 제외\n",
    "- 파생 피처(num_missing)도 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59dbfe9",
   "metadata": {},
   "source": [
    "#### 2. 모든 ind 피처 값을 연결해서 새로운 피처 만들기 -> mix_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9e92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류가 ind인 피처\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "# 피처들을 순회하면서 모든 값을 연결\n",
    "is_first_feature = True\n",
    "for ind_feature in ind_features :\n",
    "    if is_first_feature :\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str)+'_'\n",
    "        is_first_feature = False\n",
    "        \n",
    "    else :\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) +'_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c4eabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
       "1           1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
       "2          5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
       "3           0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
       "4           0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
       "                           ...                  \n",
       "1488023     0_1_6_0_0_0_1_0_0_0_0_0_0_0_2_0_0_1_\n",
       "1488024    5_3_5_1_0_0_0_1_0_0_0_0_0_0_11_1_0_0_\n",
       "1488025     0_1_5_0_0_1_0_0_0_0_0_0_0_0_5_0_0_1_\n",
       "1488026    6_1_5_1_0_0_0_0_1_0_0_0_0_0_13_1_0_0_\n",
       "1488027    7_1_4_1_0_0_0_0_1_0_0_0_0_0_12_1_0_0_\n",
       "Name: mix_ind, Length: 1488028, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249f80c",
   "metadata": {},
   "source": [
    "#### 3. 명목형 피처의 고윳값별 개수를 새로운 피처로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b871e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1079327\n",
       " 2     309747\n",
       " 3      70172\n",
       " 4      28259\n",
       "-1        523\n",
       "Name: ps_ind_02_cat, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a9f049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63724533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처의 고윳값별 개수를 파생 피처로 만들기 \n",
    "cat_count_features = []\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "    \n",
    "    cat_count_features.append(f'{feature}_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2464e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat_count',\n",
       " 'ps_ind_04_cat_count',\n",
       " 'ps_ind_05_cat_count',\n",
       " 'ps_car_01_cat_count',\n",
       " 'ps_car_02_cat_count',\n",
       " 'ps_car_03_cat_count',\n",
       " 'ps_car_04_cat_count',\n",
       " 'ps_car_05_cat_count',\n",
       " 'ps_car_06_cat_count',\n",
       " 'ps_car_07_cat_count',\n",
       " 'ps_car_08_cat_count',\n",
       " 'ps_car_09_cat_count',\n",
       " 'ps_car_10_cat_count',\n",
       " 'ps_car_11_cat_count',\n",
       " 'mix_ind_count']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f8902",
   "metadata": {},
   "source": [
    "- encoded_cat_matrix : 원-핫 인코딩된 명목형 피처\n",
    "- remaining_features : 명목형 피처와 calc 분류의 피처를 제외한 피처들(+num_missing)\n",
    "- cat_count_features : mix_ind를 포함한 명목형 피처의 고윳값별 개수 파생 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d3fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "# 필요 없는 피처들\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
    "                'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "# remaining_features, cat_count_features에서 drop_features를 제거한 데이터\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "# 데이터 합치기\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                              encoded_cat_matrix], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfebfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "x = all_data_sprs[:num_train]\n",
    "x_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12556afa",
   "metadata": {},
   "source": [
    "#### 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008d9bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "bayes_dtrain = lgb.Dataset(x_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a67762",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 범위 설정 방법\n",
    "1. 하이퍼파라미터 범위를 점점 좁히는 방법\n",
    "2. 다른 상위권 캐글러가 설정한 하이퍼파라미터 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e77c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves':(30, 40),\n",
    "               'lambda_l1':(0.7, 0.9),\n",
    "               'lambda_l2':(0.9, 1),\n",
    "               'feature_fraction':(0.6, 0.7),\n",
    "               'bagging_fraction':(0.6, 0.9),\n",
    "               'min_child_samples':(6, 10),\n",
    "               'min_child_weight':(10, 40)}\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "fixed_params = {'objective':'binary',\n",
    "               'learning_rate':0.005,\n",
    "               'bagging_freq':1,\n",
    "               'force_row_wise':True,\n",
    "               'random_state':1991}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862a202",
   "metadata": {},
   "source": [
    "#### (베이지안 최적화용) 평가지표 계산 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dccb8979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gini(y_true, y_pred):\n",
    "    # 실제값과 예측값의 크기가 서로 같은지 확인(값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    \n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    l_mid = np.linspace(1/n_samples, 1, n_samples) # 대각선 값\n",
    "    \n",
    "    # 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    l_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    g_pred = np.sum(l_mid - l_pred) # 예측값에 대한 지니계수\n",
    "    \n",
    "    # 예측이 완벽할 때 지니계수\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    l_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    g_true = np.sum(l_mid - l_true) # 예측이 완벽할 때 지니계수 \n",
    "    \n",
    "    # 정규화된 지니계수\n",
    "    return g_pred / g_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dd7895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67884aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                 bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    \n",
    "    params = {'num_leaves':int(round(num_leaves)),\n",
    "             'lambda_l1' : lambda_l1,\n",
    "             'lambda_l2' : lambda_l2,\n",
    "             'feature_fraction' : feature_fraction,\n",
    "             'bagging_fraction' : bagging_fraction,\n",
    "             'min_child_samples' : int(round(min_child_samples)),\n",
    "             'min_child_weight' : min_child_weight,\n",
    "             'feature_pre_filter' : False}\n",
    "    \n",
    "    params.update(fixed_params) # 원소 추가 \n",
    "    \n",
    "    print('하이퍼파라미터:', params)\n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=params, train_set=bayes_dtrain, num_boost_round=2500,\n",
    "                         valid_sets = bayes_dvalid, feval=gini, early_stopping_rounds=300, verbose_eval=False)\n",
    "    \n",
    "    preds = lgb_model.predict(x_valid)\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649dfa04",
   "metadata": {},
   "source": [
    "#### 최적화 수행 : 최적 예측기(최적 하이퍼파라미터 값들로 훈련된 모델)를 제공하지 않음\n",
    "- 베이지안 최적화로 찾은 하이퍼파라미터를 활용해 LightGBM 모델을 다시 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6887a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f=eval_function,  # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds, # 하이퍼파라미터 범위 \n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2372a033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2856  \u001b[0m | \u001b[0m 0.7646  \u001b[0m | \u001b[0m 0.6715  \u001b[0m | \u001b[0m 0.8206  \u001b[0m | \u001b[0m 0.9545  \u001b[0m | \u001b[0m 7.695   \u001b[0m | \u001b[0m 29.38   \u001b[0m | \u001b[0m 34.38   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.2837  \u001b[0m | \u001b[0m 0.8675  \u001b[0m | \u001b[0m 0.6964  \u001b[0m | \u001b[0m 0.7767  \u001b[0m | \u001b[0m 0.9792  \u001b[0m | \u001b[0m 8.116   \u001b[0m | \u001b[0m 27.04   \u001b[0m | \u001b[0m 39.26   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.2858  \u001b[0m | \u001b[95m 0.6213  \u001b[0m | \u001b[95m 0.6087  \u001b[0m | \u001b[95m 0.704   \u001b[0m | \u001b[95m 0.9833  \u001b[0m | \u001b[95m 9.113   \u001b[0m | \u001b[95m 36.1    \u001b[0m | \u001b[95m 39.79   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.2829  \u001b[0m | \u001b[0m 0.8978  \u001b[0m | \u001b[0m 0.6594  \u001b[0m | \u001b[0m 0.8445  \u001b[0m | \u001b[0m 0.9234  \u001b[0m | \u001b[0m 8.619   \u001b[0m | \u001b[0m 10.55   \u001b[0m | \u001b[0m 30.09   \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.2851  \u001b[0m | \u001b[0m 0.7667  \u001b[0m | \u001b[0m 0.6606  \u001b[0m | \u001b[0m 0.7738  \u001b[0m | \u001b[0m 0.9033  \u001b[0m | \u001b[0m 8.769   \u001b[0m | \u001b[0m 29.31   \u001b[0m | \u001b[0m 36.6    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.8781371569203059, 'lambda_l2': 0.9, 'feature_fraction': 0.6949291823969249, 'bagging_fraction': 0.6580770160451177, 'min_child_samples': 10, 'min_child_weight': 35.85661117604572, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2853170766671574\n",
      "\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.2853  \u001b[0m | \u001b[0m 0.6581  \u001b[0m | \u001b[0m 0.6949  \u001b[0m | \u001b[0m 0.8781  \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 9.826   \u001b[0m | \u001b[0m 35.86   \u001b[0m | \u001b[0m 32.8    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.8433793375135147, 'lambda_l2': 0.9479651949974717, 'feature_fraction': 0.6859622896374784, 'bagging_fraction': 0.8362539818721497, 'min_child_samples': 6, 'min_child_weight': 39.77484183530247, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2854766974907317\n",
      "\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.2855  \u001b[0m | \u001b[0m 0.8363  \u001b[0m | \u001b[0m 0.686   \u001b[0m | \u001b[0m 0.8434  \u001b[0m | \u001b[0m 0.948   \u001b[0m | \u001b[0m 6.002   \u001b[0m | \u001b[0m 39.77   \u001b[0m | \u001b[0m 36.8    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.7, 'lambda_l2': 0.9, 'feature_fraction': 0.7, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 27.93092783176277, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2853631089269565\n",
      "\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.2854  \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 0.7     \u001b[0m | \u001b[0m 0.7     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 27.93   \u001b[0m | \u001b[0m 30.0    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7060727349466539, 'lambda_l2': 0.9122263686184788, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 40.0, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28490600502878993\n",
      "\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.2849  \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 0.6     \u001b[0m | \u001b[0m 0.7061  \u001b[0m | \u001b[0m 0.9122  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 40.0    \u001b[0m | \u001b[0m 40.0    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3,  # 무작위로 하이퍼파라미터 탐색 횟수\n",
    "                   n_iter=6)      # 베이지안 최적화 반복 횟수  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "318d7042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6213108174593661,\n",
       " 'feature_fraction': 0.608712929970154,\n",
       " 'lambda_l1': 0.7040436794880651,\n",
       " 'lambda_l2': 0.9832619845547939,\n",
       " 'min_child_samples': 9.112627003799401,\n",
       " 'min_child_weight': 36.10036444740457,\n",
       " 'num_leaves': 39.78618342232764}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 때 하이퍼파라미터 \n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdb96c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "235d180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "max_params.update(fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e71e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6213108174593661,\n",
       " 'feature_fraction': 0.608712929970154,\n",
       " 'lambda_l1': 0.7040436794880651,\n",
       " 'lambda_l2': 0.9832619845547939,\n",
       " 'min_child_samples': 9,\n",
       " 'min_child_weight': 36.10036444740457,\n",
       " 'num_leaves': 40,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "741d383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1/폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\82109\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154239\tvalid_0's gini: 0.270944\n",
      "[200]\tvalid_0's binary_logloss: 0.153176\tvalid_0's gini: 0.275764\n",
      "[300]\tvalid_0's binary_logloss: 0.152584\tvalid_0's gini: 0.279501\n",
      "[400]\tvalid_0's binary_logloss: 0.152222\tvalid_0's gini: 0.282893\n",
      "[500]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.286058\n",
      "[600]\tvalid_0's binary_logloss: 0.151824\tvalid_0's gini: 0.288805\n",
      "[700]\tvalid_0's binary_logloss: 0.151712\tvalid_0's gini: 0.290719\n",
      "[800]\tvalid_0's binary_logloss: 0.151622\tvalid_0's gini: 0.292581\n",
      "[900]\tvalid_0's binary_logloss: 0.151552\tvalid_0's gini: 0.294212\n",
      "[1000]\tvalid_0's binary_logloss: 0.151505\tvalid_0's gini: 0.295204\n",
      "[1100]\tvalid_0's binary_logloss: 0.151471\tvalid_0's gini: 0.295909\n",
      "[1200]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.296721\n",
      "[1300]\tvalid_0's binary_logloss: 0.151414\tvalid_0's gini: 0.297335\n",
      "[1400]\tvalid_0's binary_logloss: 0.151402\tvalid_0's gini: 0.297569\n",
      "[1500]\tvalid_0's binary_logloss: 0.15139\tvalid_0's gini: 0.297881\n",
      "[1600]\tvalid_0's binary_logloss: 0.151382\tvalid_0's gini: 0.298033\n",
      "[1700]\tvalid_0's binary_logloss: 0.151376\tvalid_0's gini: 0.298238\n",
      "[1800]\tvalid_0's binary_logloss: 0.151372\tvalid_0's gini: 0.298342\n",
      "[1900]\tvalid_0's binary_logloss: 0.151369\tvalid_0's gini: 0.298371\n",
      "[2000]\tvalid_0's binary_logloss: 0.151371\tvalid_0's gini: 0.298222\n",
      "[2100]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.298463\n",
      "[2200]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.298466\n",
      "[2300]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.298415\n",
      "[2400]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.298569\n",
      "[2500]\tvalid_0's binary_logloss: 0.151361\tvalid_0's gini: 0.298542\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2458]\tvalid_0's binary_logloss: 0.151355\tvalid_0's gini: 0.29865\n",
      "폴드 1 지니계수 : 0.2986504843987991\n",
      "\n",
      "######################################## 폴드 2/폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154347\tvalid_0's gini: 0.258575\n",
      "[200]\tvalid_0's binary_logloss: 0.153338\tvalid_0's gini: 0.263768\n",
      "[300]\tvalid_0's binary_logloss: 0.152804\tvalid_0's gini: 0.267635\n",
      "[400]\tvalid_0's binary_logloss: 0.152483\tvalid_0's gini: 0.271009\n",
      "[500]\tvalid_0's binary_logloss: 0.152299\tvalid_0's gini: 0.27324\n",
      "[600]\tvalid_0's binary_logloss: 0.152157\tvalid_0's gini: 0.275756\n",
      "[700]\tvalid_0's binary_logloss: 0.15206\tvalid_0's gini: 0.277655\n",
      "[800]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.279371\n",
      "[900]\tvalid_0's binary_logloss: 0.151942\tvalid_0's gini: 0.280359\n",
      "[1000]\tvalid_0's binary_logloss: 0.151898\tvalid_0's gini: 0.281475\n",
      "[1100]\tvalid_0's binary_logloss: 0.15186\tvalid_0's gini: 0.282482\n",
      "[1200]\tvalid_0's binary_logloss: 0.151835\tvalid_0's gini: 0.283198\n",
      "[1300]\tvalid_0's binary_logloss: 0.15181\tvalid_0's gini: 0.283848\n",
      "[1400]\tvalid_0's binary_logloss: 0.151796\tvalid_0's gini: 0.284221\n",
      "[1500]\tvalid_0's binary_logloss: 0.151781\tvalid_0's gini: 0.284645\n",
      "[1600]\tvalid_0's binary_logloss: 0.15177\tvalid_0's gini: 0.284943\n",
      "[1700]\tvalid_0's binary_logloss: 0.151761\tvalid_0's gini: 0.285129\n",
      "[1800]\tvalid_0's binary_logloss: 0.151755\tvalid_0's gini: 0.28522\n",
      "[1900]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.285325\n",
      "[2000]\tvalid_0's binary_logloss: 0.151749\tvalid_0's gini: 0.285504\n",
      "[2100]\tvalid_0's binary_logloss: 0.151748\tvalid_0's gini: 0.285633\n",
      "[2200]\tvalid_0's binary_logloss: 0.151744\tvalid_0's gini: 0.285711\n",
      "[2300]\tvalid_0's binary_logloss: 0.15174\tvalid_0's gini: 0.285853\n",
      "[2400]\tvalid_0's binary_logloss: 0.15174\tvalid_0's gini: 0.28594\n",
      "[2500]\tvalid_0's binary_logloss: 0.151745\tvalid_0's gini: 0.285916\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2334]\tvalid_0's binary_logloss: 0.151736\tvalid_0's gini: 0.285929\n",
      "폴드 2 지니계수 : 0.2859292916021393\n",
      "\n",
      "######################################## 폴드 3/폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15424\tvalid_0's gini: 0.263985\n",
      "[200]\tvalid_0's binary_logloss: 0.153171\tvalid_0's gini: 0.268713\n",
      "[300]\tvalid_0's binary_logloss: 0.152574\tvalid_0's gini: 0.272773\n",
      "[400]\tvalid_0's binary_logloss: 0.152223\tvalid_0's gini: 0.275785\n",
      "[500]\tvalid_0's binary_logloss: 0.152001\tvalid_0's gini: 0.278098\n",
      "[600]\tvalid_0's binary_logloss: 0.151847\tvalid_0's gini: 0.280206\n",
      "[700]\tvalid_0's binary_logloss: 0.151748\tvalid_0's gini: 0.281603\n",
      "[800]\tvalid_0's binary_logloss: 0.151682\tvalid_0's gini: 0.282672\n",
      "[900]\tvalid_0's binary_logloss: 0.151637\tvalid_0's gini: 0.283423\n",
      "[1000]\tvalid_0's binary_logloss: 0.151608\tvalid_0's gini: 0.283963\n",
      "[1100]\tvalid_0's binary_logloss: 0.151589\tvalid_0's gini: 0.284105\n",
      "[1200]\tvalid_0's binary_logloss: 0.151574\tvalid_0's gini: 0.284387\n",
      "[1300]\tvalid_0's binary_logloss: 0.151575\tvalid_0's gini: 0.284318\n",
      "[1400]\tvalid_0's binary_logloss: 0.151572\tvalid_0's gini: 0.284372\n",
      "[1500]\tvalid_0's binary_logloss: 0.151569\tvalid_0's gini: 0.284466\n",
      "[1600]\tvalid_0's binary_logloss: 0.151574\tvalid_0's gini: 0.284435\n",
      "[1700]\tvalid_0's binary_logloss: 0.151579\tvalid_0's gini: 0.284362\n",
      "Early stopping, best iteration is:\n",
      "[1478]\tvalid_0's binary_logloss: 0.151568\tvalid_0's gini: 0.284492\n",
      "폴드 3 지니계수 : 0.2844916047790675\n",
      "\n",
      "######################################## 폴드 4/폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154327\tvalid_0's gini: 0.256916\n",
      "[200]\tvalid_0's binary_logloss: 0.15331\tvalid_0's gini: 0.261871\n",
      "[300]\tvalid_0's binary_logloss: 0.152761\tvalid_0's gini: 0.265441\n",
      "[400]\tvalid_0's binary_logloss: 0.152441\tvalid_0's gini: 0.268613\n",
      "[500]\tvalid_0's binary_logloss: 0.152245\tvalid_0's gini: 0.271168\n",
      "[600]\tvalid_0's binary_logloss: 0.152098\tvalid_0's gini: 0.273746\n",
      "[700]\tvalid_0's binary_logloss: 0.152012\tvalid_0's gini: 0.275192\n",
      "[800]\tvalid_0's binary_logloss: 0.151952\tvalid_0's gini: 0.276278\n",
      "[900]\tvalid_0's binary_logloss: 0.151911\tvalid_0's gini: 0.277039\n",
      "[1000]\tvalid_0's binary_logloss: 0.151871\tvalid_0's gini: 0.277996\n",
      "[1100]\tvalid_0's binary_logloss: 0.151844\tvalid_0's gini: 0.278535\n",
      "[1200]\tvalid_0's binary_logloss: 0.151827\tvalid_0's gini: 0.279055\n",
      "[1300]\tvalid_0's binary_logloss: 0.151817\tvalid_0's gini: 0.27936\n",
      "[1400]\tvalid_0's binary_logloss: 0.151799\tvalid_0's gini: 0.279872\n",
      "[1500]\tvalid_0's binary_logloss: 0.151797\tvalid_0's gini: 0.280053\n",
      "[1600]\tvalid_0's binary_logloss: 0.151792\tvalid_0's gini: 0.280148\n",
      "[1700]\tvalid_0's binary_logloss: 0.151794\tvalid_0's gini: 0.280162\n",
      "[1800]\tvalid_0's binary_logloss: 0.151793\tvalid_0's gini: 0.280319\n",
      "[1900]\tvalid_0's binary_logloss: 0.151795\tvalid_0's gini: 0.280422\n",
      "[2000]\tvalid_0's binary_logloss: 0.151797\tvalid_0's gini: 0.280419\n",
      "[2100]\tvalid_0's binary_logloss: 0.151799\tvalid_0's gini: 0.280516\n",
      "Early stopping, best iteration is:\n",
      "[1852]\tvalid_0's binary_logloss: 0.15179\tvalid_0's gini: 0.280514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴드 4 지니계수 : 0.2805136229288192\n",
      "\n",
      "######################################## 폴드 5/폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15439\tvalid_0's gini: 0.26681\n",
      "[200]\tvalid_0's binary_logloss: 0.15338\tvalid_0's gini: 0.272186\n",
      "[300]\tvalid_0's binary_logloss: 0.152821\tvalid_0's gini: 0.275897\n",
      "[400]\tvalid_0's binary_logloss: 0.1525\tvalid_0's gini: 0.278734\n",
      "[500]\tvalid_0's binary_logloss: 0.152277\tvalid_0's gini: 0.282151\n",
      "[600]\tvalid_0's binary_logloss: 0.15212\tvalid_0's gini: 0.285039\n",
      "[700]\tvalid_0's binary_logloss: 0.152009\tvalid_0's gini: 0.287435\n",
      "[800]\tvalid_0's binary_logloss: 0.15192\tvalid_0's gini: 0.289549\n",
      "[900]\tvalid_0's binary_logloss: 0.151862\tvalid_0's gini: 0.290886\n",
      "[1000]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.291935\n",
      "[1100]\tvalid_0's binary_logloss: 0.151782\tvalid_0's gini: 0.292972\n",
      "[1200]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.293784\n",
      "[1300]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.294315\n",
      "[1400]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.294475\n",
      "[1500]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.294786\n",
      "[1600]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.295146\n",
      "[1700]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295268\n",
      "[1800]\tvalid_0's binary_logloss: 0.151695\tvalid_0's gini: 0.295212\n",
      "[1900]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295454\n",
      "[2000]\tvalid_0's binary_logloss: 0.151693\tvalid_0's gini: 0.2954\n",
      "[2100]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295427\n",
      "[2200]\tvalid_0's binary_logloss: 0.151692\tvalid_0's gini: 0.295538\n",
      "[2300]\tvalid_0's binary_logloss: 0.151699\tvalid_0's gini: 0.295411\n",
      "Early stopping, best iteration is:\n",
      "[2045]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295553\n",
      "폴드 5 지니계수 : 0.29555250456072807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "oof_val_preds = np.zeros(x.shape[0])\n",
    "oof_test_preds = np.zeros(x_test.shape[0])\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(x, y)):\n",
    "    print('#'*40, f'폴드 {idx+1}/폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    x_train, y_train = x[train_idx], y[train_idx]\n",
    "    x_valid, y_valid = x[valid_idx], y[valid_idx]\n",
    "    \n",
    "    dtrain = lgb.Dataset(x_train, y_train)\n",
    "    dvalid = lgb.Dataset(x_valid, y_valid)\n",
    "    \n",
    "    lgb_model = lgb.train(params = max_params, train_set = dtrain, num_boost_round=2500,\n",
    "                         valid_sets = dvalid, feval=gini, early_stopping_rounds=300, verbose_eval=100)\n",
    "    \n",
    "    oof_test_preds += lgb_model.predict(x_test)/folds.n_splits\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(x_valid)\n",
    "    \n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30ca2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof 검증 데이터 지니계수 : 0.2889651000887542\n"
     ]
    }
   ],
   "source": [
    "print('oof 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97e7cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv(r\"C:\\Users\\82109\\OneDrive\\바탕 화면\\python study\\kaggle\\porto seguro\\sample_submission2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684d594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541da84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d8ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
